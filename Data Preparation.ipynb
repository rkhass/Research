{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "The main goal of the following class is an effective way of feature engineering. Since this dataset has lots of logs of users, then some features have constant values per user, but some features are variable for one user. \n",
    "\n",
    "Firstly, during a feature engineering we have to be very accurate to avoid any sorts of leakage, which can appear accidently, when you prepare your data. Secondly, we need to do a lot of tests of our features for developing good model. Due to the size of data, I need to optimize steps of my feature engineering and constructing datasets to fit model, because time of feature processing takes lots of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run Feature_processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_pickle('../Data_original/All_Data.pkl')\n",
    "targets = pd.read_pickle(\"../Data_original/Targets.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, data, target):\n",
    "        self.data = self._clear_data(data)\n",
    "        self.target = target[['ID', 'target']]\n",
    "        self._get_train_test_split()\n",
    "        \n",
    "    def _clear_data(self, data):\n",
    "        message = 'Input data of shape ' + str(data.shape) + ' converted to data of shape '\n",
    "        data = data[data.KORREKTUR>=0.][data.BETRAG>=0.]\n",
    "        \n",
    "        data = self._is_sorted_data_by_id(data) # check whether data is sorted by id\n",
    "        \n",
    "        message += str(data.shape)\n",
    "        print(message)\n",
    "        return data\n",
    "    \n",
    "    def _separate_into_train_test(self, data):\n",
    "        train, test = data[data.ID.isin(self.id_train)], data[data.ID.isin(self.id_test)]\n",
    "        return train, test \n",
    "    \n",
    "    def _is_sorted_data_by_id(self, data):\n",
    "        list_of_id = data.ID.apply(lambda dt: int(dt[3:])).values\n",
    "        bools = list_of_id[:-1] > list_of_id[1:]\n",
    "                \n",
    "        if np.sum(bools) > 0:\n",
    "            print('Data is not sorted')\n",
    "            data = self._sort_data_by_id(data)\n",
    "            print('Data is sorted now')\n",
    "            return data\n",
    "        else:\n",
    "            return data\n",
    "                    \n",
    "    def _sort_data_by_id(self, data):\n",
    "        data['ID_int'] = data.ID.apply(lambda dt: int(dt[3:]))\n",
    "        data = data.sort_values('ID_int')\n",
    "        return data.drop(['ID_int'], axis=1)\n",
    "    \n",
    "    def _get_train_test_split(self, test_ID_size=0.2, test_Time_size=0.25):\n",
    "        \n",
    "        assert 0 < test_ID_size < 1 or 0 < test_Time_size < 1, 'Test size must be between 0 and 1'\n",
    "        \n",
    "        train, test = separate_data(self.data, self.target, \n",
    "                                    test_ID_size=test_ID_size, \n",
    "                                    test_Time_size=test_Time_size)\n",
    "        \n",
    "\n",
    "        self.id_train = sort_indices(train.ID.unique()) \n",
    "        self.id_test  = sort_indices(test.ID.unique()) \n",
    "        \n",
    "        self.df_id_train = pd.DataFrame(self.id_train, columns=['ID'])\n",
    "        self.df_id_test  = pd.DataFrame(self.id_test , columns=['ID'])\n",
    "        \n",
    "        self.X_train = train\n",
    "        self.X_test  = test\n",
    "        \n",
    "        return train, test\n",
    "    \n",
    "            \n",
    "    def fit_transform_tf_idf(self, train_sent, test_sent, name):\n",
    "        model_tf_idf = TfidfVectorizer()\n",
    "        model_tf_idf.fit(train_sent)\n",
    "        X_train, X_test = model_tf_idf.transform(train_sent), model_tf_idf.transform(test_sent) \n",
    "        \n",
    "        names = [name + '_' + str(i) for i in range(X_train.shape[1])]\n",
    "        \n",
    "        df_train = pd.SparseDataFrame(X_train, columns=names).fillna(0)\n",
    "        df_train = pd.concat([self.df_id_train, df_train], axis=1)\n",
    "        \n",
    "        df_test = pd.SparseDataFrame(X_test, columns=names).fillna(0)\n",
    "        df_test = pd.concat([self.df_id_test, df_test], axis=1)\n",
    "        \n",
    "        return df_train, df_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def simple_features(self, save=False):\n",
    "        data = self.data.drop_duplicates(subset=['ID'])[['ID', 'RECHNUNGSBETRAG', 'ALTER', 'GESCHLECHT', 'VERSICHERUNG']].reset_index(drop=True)        \n",
    "        data.RECHNUNGSBETRAG = np.log(data.RECHNUNGSBETRAG - np.min(data.RECHNUNGSBETRAG, 0) +  1)\n",
    "        \n",
    "        train, test = self._separate_into_train_test(data)\n",
    "        if save: save_features(train, test, 'simple_features')\n",
    "            \n",
    "    def transform_betrag_histogram(self, save=False, name='betrag_histogram'):\n",
    "        \n",
    "        bins, _ = adaptive_boards(self.data.BETRAG.sort_values().values)\n",
    "        \n",
    "        def get_num(value):    \n",
    "            for i in range(len(bins)-1):\n",
    "                if bins[i] > value:\n",
    "                    return i-1    \n",
    "            return len(bins)\n",
    "        \n",
    "        self.X_train['b_hist'] = self.X_train.BETRAG.apply(lambda dt: str(get_num(dt)))\n",
    "        self.X_test ['b_hist'] =  self.X_test.BETRAG.apply(lambda dt: str(get_num(dt)))\n",
    "        \n",
    "        train = construct_sentences(self.X_train, 'b_hist', dropna=False)\n",
    "        test = construct_sentences(self.X_test, 'b_hist', dropna=False)\n",
    "        \n",
    "        train, test = self.fit_transform_tf_idf(train, test, 'betrag')   \n",
    "                \n",
    "        if save: save_features(train, test, 'betrag_histogram')\n",
    "            \n",
    "\n",
    "    def transform_faktor_histogram(self, save=False, name='betrag_histogram'):\n",
    "        \n",
    "        bins, _ = adaptive_boards(self.data.FAKTOR.sort_values().values, N=25)\n",
    "        def get_num(value):    \n",
    "            for i in range(len(bins)-1):\n",
    "                if bins[i] > value:\n",
    "                    return i-1    \n",
    "            return len(bins)\n",
    "        \n",
    "        self.X_train['f_hist'] = self.X_train.FAKTOR.apply(lambda dt: str(get_num(dt)))\n",
    "        self.X_test ['f_hist'] =  self.X_test.FAKTOR.apply(lambda dt: str(get_num(dt)))\n",
    "        \n",
    "        train = construct_sentences(self.X_train, 'f_hist', dropna=False)\n",
    "        test = construct_sentences(self.X_test, 'f_hist', dropna=False)\n",
    "        \n",
    "        train, test = self.fit_transform_tf_idf(train, test, 'faktor')\n",
    "        if save: save_features(train, test, 'faktor_histogram')\n",
    "        \n",
    "    def transform_anzahl_histogram(self, save=False, name='anzahl_histogram'):\n",
    "        \n",
    "        bins, _ = adaptive_boards(self.data.ANZAHL.sort_values().values, N=80)\n",
    "        def get_num(value):    \n",
    "            for i in range(len(bins)-1):\n",
    "                if bins[i] > value:\n",
    "                    return i-1    \n",
    "            return len(bins)\n",
    "        \n",
    "        self.X_train['a_hist'] = self.X_train.ANZAHL.apply(lambda dt: str(get_num(dt)))\n",
    "        self.X_test ['a_hist'] =  self.X_test.ANZAHL.apply(lambda dt: str(get_num(dt)))\n",
    "        \n",
    "        train = construct_sentences(self.X_train, 'a_hist', dropna=False)\n",
    "        test = construct_sentences(self.X_test, 'a_hist', dropna=False)\n",
    "        \n",
    "        train, test = self.fit_transform_tf_idf(train, test, 'anzahl')\n",
    "        if save: save_features(train, test, 'anzahl_histogram')\n",
    "\n",
    "    def transform_faktor_stats(self, save=False, name='faktor_stats'):\n",
    "        faktor_mean = self.data.groupby(['ID'])['FAKTOR'].mean()\n",
    "        faktor_std = self.data.groupby(['ID'])['FAKTOR'].std().fillna(0)\n",
    "        faktor_min = self.data.groupby(['ID'])['FAKTOR'].min()\n",
    "        faktor_max = self.data.groupby(['ID'])['FAKTOR'].max()\n",
    "        faktor_median = self.data.groupby(['ID'])['FAKTOR'].median()\n",
    "\n",
    "        faktor_all = pd.concat([faktor_mean, faktor_std, faktor_min, faktor_max, faktor_median], axis=1, keys=\n",
    "                  ['FAKTOR_mean', 'FAKTOR_std', 'FAKTOR_min', 'FAKTOR_max', 'FAKTOR_median']).reset_index()\n",
    "    \n",
    "        \n",
    "        train, test = self._separate_into_train_test(faktor_all)\n",
    "        if save: save_features(train, test, name)\n",
    "\n",
    "        return faktor_all\n",
    "    \n",
    "    def transform_typ_stats(self, save=False, name='typ_stats'):\n",
    "        self.data.TYP.fillna(-1, inplace=True)\n",
    "        typ_mean = self.data.groupby(['ID'])['TYP'].mean()\n",
    "        typ_std = self.data.groupby(['ID'])['TYP'].std().fillna(0)\n",
    "        typ_min = self.data.groupby(['ID'])['TYP'].min()\n",
    "        typ_max = self.data.groupby(['ID'])['TYP'].max()\n",
    "        typ_median = self.data.groupby(['ID'])['TYP'].median()\n",
    "\n",
    "        typ_all = pd.concat([typ_mean, typ_std, typ_min, typ_max, typ_median], axis=1, keys=\n",
    "                  ['TYP_mean', 'TYP_std', 'TYP_min', 'TYP_max', 'TYP_median']).reset_index()\n",
    "        \n",
    "        train, test = self._separate_into_train_test(typ_all)\n",
    "        if save: save_features(train, test, name)\n",
    "            \n",
    "        return typ_all\n",
    "    \n",
    "    def transform_betrag_stats(self, save=False, name='betrag_stats'):\n",
    "        normed_values = self.data.BETRAG / self.data.ANZAHL\n",
    "        self.data['BETRAG_normed_log'] = np.log(normed_values - np.min(normed_values, 0) +  1)\n",
    "\n",
    "        betrag_mean = self.data.groupby(['ID'])['BETRAG_normed_log'].mean()\n",
    "        betrag_std = self.data.groupby(['ID'])['BETRAG_normed_log'].std().fillna(0)\n",
    "        betrag_min = self.data.groupby(['ID'])['BETRAG_normed_log'].min()\n",
    "        betrag_max = self.data.groupby(['ID'])['BETRAG_normed_log'].max()\n",
    "        betrag_median = self.data.groupby(['ID'])['BETRAG_normed_log'].median()\n",
    "\n",
    "        betrag_all = pd.concat([betrag_mean, betrag_std, betrag_min, betrag_max, betrag_median], axis=1, keys=\n",
    "                  ['BETRAG_mean', 'BETRAG_std', 'BETRAG_min', 'BETRAG_max', 'BETRAG_median']).reset_index()\n",
    "        \n",
    "        train, test = self._separate_into_train_test(betrag_all)\n",
    "        if save: save_features(train, test, name)\n",
    "            \n",
    "        return betrag_all\n",
    "    \n",
    "    def transform_nummer_tf_idf(self, save=False):\n",
    "        train = construct_sentences(self.X_train, 'NUMMER', dropna=False)\n",
    "        test = construct_sentences(self.X_test, 'NUMMER', dropna=False)\n",
    "\n",
    "        train, test = self.fit_transform_tf_idf(train, test, 'nummer')   \n",
    "        \n",
    "        if save: save_features(train, test, 'nummer_tfidf')\n",
    "\n",
    "    def transform_nummer_kat_tf_idf(self, save=False):\n",
    "        train = construct_sentences(self.X_train, 'NUMMER_KAT', dropna=False)\n",
    "        test = construct_sentences(self.X_test, 'NUMMER_KAT', dropna=False)\n",
    "\n",
    "        train, test = self.fit_transform_tf_idf(train, test, 'nummer_kat')   \n",
    "        \n",
    "        if save: save_features(train, test, 'nummer_kat_tfidf')\n",
    "        \n",
    "    def transform_art_tf_idf(self, save=False):\n",
    "        train = construct_sentences(self.X_train, 'ART', dropna=False)\n",
    "        test = construct_sentences(self.X_test, 'ART', dropna=False)\n",
    "\n",
    "        train, test = self.fit_transform_tf_idf(train, test, 'art')   \n",
    "        \n",
    "        if save: save_features(train, test, 'art_tfidf')\n",
    "\n",
    "    def transform_leistung_tf_idf(self, save=False):\n",
    "        train = construct_sentences(self.X_train, 'LEISTUNG', dropna=False)\n",
    "        test = construct_sentences(self.X_test, 'LEISTUNG', dropna=False)\n",
    "\n",
    "        train, test = self.fit_transform_tf_idf(train, test, 'leistung')   \n",
    "        \n",
    "        if save: save_features(train, test, 'leistung_tfidf')\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        self.simple_features(save=True)\n",
    "        self.transform_betrag_histogram(save=True)\n",
    "        self.transform_faktor_histogram(save=True)\n",
    "        self.transform_anzahl_histogram(save=True)\n",
    "        self.transform_faktor_stats(save=True)\n",
    "        self.transform_typ_stats(save=True)\n",
    "        self.transform_betrag_stats(save=True)\n",
    "        self.transform_nummer_tf_idf(save=True)\n",
    "        self.transform_nummer_kat_tf_idf(save=True)\n",
    "        self.transform_art_tf_idf(save=True)\n",
    "        self.transform_leistung_tf_idf(save=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data of shape (3275027, 17) converted to data of shape (3275022, 17)\n",
      "Average length of records in Train data:  6.63\n",
      "Average length of records in Test  data:  2.93\n",
      "35.0% of data was dropped\n"
     ]
    }
   ],
   "source": [
    "d = Data(DATA, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a2ce6704404265ae82baa16fc6c1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=295974), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe275fb012a42378fc9a284b218dea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56179), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c87041067d4c92964cc0a776a0f35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=295974), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b997abd8fc4c7f9a7d51ab3a17eff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56179), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19aafd9a6e446f8ba6b2219122285f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=295974), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8bb43a4e144118bbbf608133ff0b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56179), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc81f26903a742868a6caad34d7e2e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=295974), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bafcd2bad24d2a8c44ed310add9fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56179), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efe49b8a8b04d009c14e1d8821d2ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=295974), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650f7a34c3354ffcaa75f20d789be9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56179), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78375b36473478996e7b50cfef5fe28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=295974), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5037275db54394afcbddfc055a32fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56179), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d891de1787747d7a2bfdc99e3cebbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=295974), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbe1a0a12dd4872a279d796fce67e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56179), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 17min 28s, sys: 36.1 s, total: 18min 4s\n",
      "Wall time: 17min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm:\n",
    "    1. Randomly split ID-s of train and test into 2 parts\n",
    "    2. Choose separator to devide past (train data) and future (test data).\n",
    "    \n",
    "     All    |+|-| < Train\n",
    "    data    -----\n",
    "            |-|+| < Test\n",
    "            \n",
    "          ---------> \n",
    "             Time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
