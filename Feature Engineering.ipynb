{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from time import time\n",
    "\n",
    "from imblearn.ensemble import BalanceCascade \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subdata(data, size=10000):\n",
    "    column = ['KORREKTUR']\n",
    "    if data.KORREKTUR.dtype != np.float32:\n",
    "        korrektur_vals = data[column].apply(lambda x: x.str.replace(',', '.'))[column[0]]\n",
    "    else:\n",
    "        korrektur_vals = data[column[0]]\n",
    "    korrektur_vals = pd.to_numeric(korrektur_vals, downcast='float', errors='ignore')\n",
    "\n",
    "    korrektur_df = pd.concat([data.ID, korrektur_vals], axis=1)\n",
    "\n",
    "    target =  korrektur_df.drop_duplicates(subset=['ID'])[['ID', 'KORREKTUR']].reset_index(drop=True)\n",
    "    N = len(target)\n",
    "\n",
    "    ratio = np.sign(target.KORREKTUR).mean()\n",
    "    n_samples = int(ratio * size)\n",
    "    indices1 = np.random.choice(target[target.KORREKTUR>0].ID, size = n_samples, replace=False)\n",
    "    indices0 = np.random.choice(target[target.KORREKTUR==0].ID, size = size - n_samples, replace=False)\n",
    "\n",
    "    indices = np.append(indices0, indices1)\n",
    "    return data.set_index('ID').loc[indices].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import data\n",
    "DATA = pd.read_csv(\"Data/daten_anonym2/arzta_daten_anonym1.csv\", sep=';')\n",
    "print(DATA.shape)\n",
    "\n",
    "# DATA = get_subdata(DATA, size=10000)\n",
    "# print(DATA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Same** for each ID: *RECHNUNGSBETRAG, ALTER, GESCHLECHT, VERSICHERUNG == FACHRICHTUNG* (4)\n",
    "* **Diff** for each ID: *NUMMER, NUMMER_KAT, ANZAHL, FAKTOR, BETRAG, ART, TYP, LEISTUNG* (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "## transfrom str type to float type\n",
    "columns_comma = ['RECHNUNGSBETRAG', 'FAKTOR', 'BETRAG', 'ALTER', 'KORREKTUR'] \n",
    "DATA[columns_comma] = DATA[columns_comma].apply(lambda x: x.str.replace(',', '.'))\n",
    "for column in columns_comma:\n",
    "    DATA[column] = pd.to_numeric(DATA[column], downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## construct target column\n",
    "target = DATA.groupby(['ID'])['KORREKTUR'].apply(lambda dt: int(np.sign(dt).values[0])).to_frame(name='target')\n",
    "DATA = DATA.merge(target, on='ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA must be sorted by Id\n",
    "current_id = ''\n",
    "indices = []\n",
    "\n",
    "for i, Id in enumerate(DATA.ID):\n",
    "    if Id != current_id:\n",
    "        if i > 0: \n",
    "            indices.append(current_index)\n",
    "        current_id = Id\n",
    "        current_index = [i]\n",
    "    else:\n",
    "        current_index.append(i)\n",
    "        \n",
    "indices.append(current_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BETRAG, ANZAHL: mean, std, min, max, median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "sns.distplot(DATA.BETRAG)\n",
    "plt.subplot(122)\n",
    "sns.distplot(np.log(DATA.BETRAG  - DATA.BETRAG.min() + 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETRAG / ANZAHL (prices for one treatment) \n",
    "vals = DATA.BETRAG / DATA.ANZAHL\n",
    "DATA.BETRAG = np.log(vals - np.min(vals, 0) +  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# construct mean and std values for each ID\n",
    "betrag_mean = DATA.groupby(['ID'])['BETRAG'].mean()\n",
    "betrag_std = DATA.groupby(['ID'])['BETRAG'].std().fillna(0)\n",
    "betrag_min = DATA.groupby(['ID'])['BETRAG'].min()\n",
    "betrag_max = DATA.groupby(['ID'])['BETRAG'].max()\n",
    "betrag_median = DATA.groupby(['ID'])['BETRAG'].median()\n",
    "\n",
    "betrag_all = pd.concat([betrag_mean, betrag_std, betrag_min, betrag_max, betrag_median], axis=1, keys=\n",
    "          ['BETRAG_mean', 'BETRAG_std', 'BETRAG_min', 'BETRAG_max', 'BETRAG_median']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betrag_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "betrag_data = []\n",
    "start = time()\n",
    "ticks = np.linspace(2, 6, 100)\n",
    "ids = DATA.ID.unique()\n",
    "for i, Id in enumerate(ids):\n",
    "    indices[i]\n",
    "    betrag_data.append(np.histogram(DATA.iloc[indices[i]]['BETRAG'], ticks)[0])\n",
    "    if i % 3000 == 0: \n",
    "        print(i, 'Done', np.round(i / len(ids) * 100, 2) , '% time:', time() - start)    \n",
    "\n",
    "betrag_data = pd.DataFrame(betrag_data, columns = ['betrag' + str(np.round(val, 2)) for val in ticks[:-1]])\n",
    "betrag_data = pd.concat([pd.Series(ids, name='ID'), betrag_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betrag_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAKTOR: mean, std, min, max, median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# construct mean and std values for each ID\n",
    "faktor_mean = DATA.groupby(['ID'])['FAKTOR'].mean()\n",
    "faktor_std = DATA.groupby(['ID'])['FAKTOR'].std().fillna(0)\n",
    "faktor_min = DATA.groupby(['ID'])['FAKTOR'].min()\n",
    "faktor_max = DATA.groupby(['ID'])['FAKTOR'].max()\n",
    "faktor_median = DATA.groupby(['ID'])['FAKTOR'].median()\n",
    "\n",
    "faktor_all = pd.concat([faktor_mean, faktor_std, faktor_min, faktor_max, faktor_median], axis=1, keys=\n",
    "          ['FAKTOR_mean', 'FAKTOR_std', 'FAKTOR_min', 'FAKTOR_max', 'FAKTOR_median']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faktor_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TYP: mean, std, min, max, median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# construct mean and std values for each ID\n",
    "DATA.TYP.fillna(-1, inplace=True)\n",
    "typ_mean = DATA.groupby(['ID'])['TYP'].mean()\n",
    "typ_std = DATA.groupby(['ID'])['TYP'].std().fillna(0)\n",
    "typ_min = DATA.groupby(['ID'])['TYP'].min()\n",
    "typ_max = DATA.groupby(['ID'])['TYP'].max()\n",
    "typ_median = DATA.groupby(['ID'])['TYP'].median()\n",
    "\n",
    "typ_all = pd.concat([typ_mean, typ_std, typ_min, typ_max, typ_median], axis=1, keys=\n",
    "          ['TYP_mean', 'TYP_std', 'TYP_min', 'TYP_max', 'TYP_median']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUMMER, NUMMER_KAT, LEISTUNG, ART. Encode all treatments and their categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nummer_list = []\n",
    "\n",
    "ids = DATA.ID.unique()\n",
    "start = time()\n",
    "for i, Id in enumerate(ids):\n",
    "    nummer_counter = DATA.iloc[indices[i]]['NUMMER'].value_counts()\n",
    "    nummer_list.append(dict(nummer_counter))\n",
    "    if i % 3000 == 0: \n",
    "        print(i, 'Done', np.round(i / len(ids) * 100, 2) , '% time:', time() - start)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 100\n",
    "# nummer_list_size = int(len(nummer_list) / (batch_size - 1))\n",
    "\n",
    "# start = time()\n",
    "# for i in range(batch_size):\n",
    "#     batch = nummer_list[i * nummer_list_size : (i + 1) * nummer_list_size]\n",
    "#     if i == 0:    \n",
    "#         pd.DataFrame(batch, columns=DATA.NUMMER.unique()).to_csv('NummerData.csv', index=False)\n",
    "#     else:\n",
    "#         with open('NummerData.csv', 'a') as csvFile:\n",
    "#             writer = csv.writer(csvFile)\n",
    "#             writer.writerows(pd.DataFrame(batch, columns=DATA.NUMMER.unique()).values)\n",
    "\n",
    "#         csvFile.close()\n",
    "#     print(i, 'Done,', ' time:', time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# chunksize = 1000\n",
    "# chunks = pd.read_csv('NummerData.csv', chunksize=chunksize)\n",
    "\n",
    "# nummer_data = []\n",
    "# start = time()\n",
    "# for i, chunk in enumerate(chunks):\n",
    "#     nummer_data.append(chunk.to_sparse())\n",
    "#     print(i, 'Done,', ' time:', time() - start)\n",
    "    \n",
    "# nummer_df = pd.concat(nummer_data)\n",
    "\n",
    "# nummer_df = pd.read_csv('NummerData.csv')\n",
    "# nummer_df = nummer_df.to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Attention: it takes a lot of memory\n",
    "\n",
    "nummer_df = pd.DataFrame(nummer_list).to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nummer_kat_list = []\n",
    "leistung_list = []\n",
    "art_list = []\n",
    "\n",
    "start = time()\n",
    "ids = DATA.ID.unique()\n",
    "for i, Id in enumerate(ids):\n",
    "\n",
    "    nummer_kat_counter = DATA.iloc[indices[i]]['NUMMER_KAT'].value_counts()\n",
    "    leistung_counter = DATA.iloc[indices[i]]['LEISTUNG'].value_counts()\n",
    "    art_counter = DATA.iloc[indices[i]]['ART'].value_counts()\n",
    "    \n",
    "    nummer_kat_list.append(dict(nummer_kat_counter))\n",
    "    leistung_list.append(dict(leistung_counter))\n",
    "    art_list.append(dict(art_counter))\n",
    "    \n",
    "    if i % 3000 == 0: \n",
    "        print(i, 'Done', np.round(i / len(ids) * 100, 2) , '% time:', time() - start)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "nummer_kat_df = pd.DataFrame(nummer_kat_list).to_sparse()\n",
    "leistung_df = pd.DataFrame(leistung_list).to_sparse()\n",
    "art_df = pd.DataFrame(art_list).to_sparse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "def plot_confusion_matrix(prediction, y_test):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    c_matrix = confusion_matrix(y_test, prediction)\n",
    "    c_matrix_ = np.round(100 * c_matrix / c_matrix.sum(axis=1).reshape(-1, 1))\n",
    "    plt.subplot(121)\n",
    "    sns.heatmap(c_matrix, annot=True, fmt=\"d\");\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.subplot(122)\n",
    "    sns.heatmap(np.asarray(c_matrix_, dtype=int), annot=True, fmt=\"d\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def get_roc_auc_score(prediction, y_test):\n",
    "    value = np.round(roc_auc_score(y_test, prediction[:,1]),4)\n",
    "    print('ROC AUC score:',  value)\n",
    "    \n",
    "    return value\n",
    "\n",
    "def get_pr_auc_score(prediction, y_test):\n",
    "    value = np.round(average_precision_score(y_test, prediction[:, 1]),4)\n",
    "    print('PR AUC score:',  value)\n",
    "    \n",
    "    return value\n",
    "    \n",
    "def plot_curves(prediction, y_test):\n",
    "    tpr, fpr, _ = roc_curve(y_test, prediction[:,1])\n",
    "    roc_auc = roc_auc_score(y_test, prediction[:,1])\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, prediction[:,1])\n",
    "    average_precision = average_precision_score(y_test, prediction[:,1])\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.subplot(121)\n",
    "    plt.step(tpr, fpr, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(tpr, fpr, step='post', alpha=0.2, color='b')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class ROC curve: ROC AUC={0:0.2f}'.format(roc_auc))\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "    \n",
    "def get_threshold_by_f1(prediction_proba, y_test, plot=False):\n",
    "    thresholds = np.linspace(prediction_proba[:, 1].min(), prediction_proba[:, 1].max(), 300)\n",
    "    curve = []\n",
    "    for val in thresholds:\n",
    "       \n",
    "        prediction = np.asarray((prediction_proba[:, 1] >= val), int)\n",
    "        curve.append(f1_score(prediction, y_test))\n",
    "\n",
    "    argmax = np.argmax(curve)\n",
    "\n",
    "    if plot == True:\n",
    "        plt.figure(figsize=(14, 4))\n",
    "        plt.plot(thresholds, curve)\n",
    "        plt.plot([thresholds[argmax]] * 2, [np.min(curve), np.max(curve)])\n",
    "    \n",
    "    return thresholds[argmax]\n",
    "\n",
    "def plot_probas(prediction_proba, y_test):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    probas = prediction_proba[:, 1]\n",
    "    ax = sns.distplot(probas)\n",
    "    proba_mean = probas.mean()\n",
    "    proba_f1_best = get_threshold_by_f1(prediction_proba, y_test)\n",
    "    plt.plot([proba_mean] * 2, ax.get_ylim(), label='Mean of probabilites')\n",
    "    plt.plot([proba_f1_best] * 2, ax.get_ylim(), label='Best threshold by f1 score')\n",
    "    plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DATA.drop_duplicates(subset=['ID'])[['ID', 'RECHNUNGSBETRAG', 'ALTER', 'GESCHLECHT', 'VERSICHERUNG', 'target']].reset_index(drop=True)\n",
    "\n",
    "data = data.merge(betrag_all, on='ID', how='inner')\n",
    "data = data.merge(betrag_data, on='ID', how='inner')\n",
    "data = data.merge(typ_all, on='ID', how='inner')\n",
    "data = data.merge(faktor_all, on='ID', how='inner')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# nummer_df.fillna(0, inplace=True)\n",
    "# nummer_kat_df.fillna(0, inplace=True)\n",
    "# leistung_df.fillna(0, inplace=True)\n",
    "# art_df.fillna(0, inplace=True)\n",
    "\n",
    "data = pd.concat([data, nummer_df], axis=1)\n",
    "data = pd.concat([data, nummer_kat_df], axis=1)\n",
    "data = pd.concat([data, leistung_df], axis=1)\n",
    "data = pd.concat([data, art_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_csv(data):\n",
    "    batch_size = 100\n",
    "    data_size = int(len(data) / (batch_size - 1))\n",
    "\n",
    "    start = time()\n",
    "    for i in range(batch_size):\n",
    "        batch = data.iloc[i * data_size : (i + 1) * data_size]\n",
    "        if i == 0:    \n",
    "            batch.to_csv('TransformedData.csv', index=False)\n",
    "        else:\n",
    "            with open('TransformedData.csv', 'a') as csvFile:\n",
    "                writer = csv.writer(csvFile)\n",
    "                writer.writerows(batch.values)\n",
    "\n",
    "            csvFile.close()\n",
    "        print(i, 'Done,', ' time:', time() - start)\n",
    "        \n",
    "def read_csv(filename='TransformedData.csv'):\n",
    "    chunksize = 10000\n",
    "    chunks = pd.read_csv(filename, chunksize=chunksize)\n",
    "\n",
    "    data = []\n",
    "    start = time()\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        data.append(chunk.to_sparse())\n",
    "        print(i, 'Done,', ' time:', time() - start)\n",
    "\n",
    "    return pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv('TransformedData.csv')\n",
    "#data = data.to_sparse()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
